{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# EPIC No-Show Prediction Model: Evaluation and Analysis\n",
    "\n",
    "## Introduction & Rationale\n",
    "\n",
    "This notebook provides a comprehensive evaluation of a pre-built machine learning model designed to predict the likelihood of patient no-shows for scheduled medical appointments. The primary goal is to assess the model's accuracy, reliability, and practical utility in a clinical setting.\n",
    "\n",
    "By analyzing the model's predicted probabilities against actual appointment outcomes, we can determine its effectiveness in distinguishing between patients who are likely to attend their appointments and those who are at high risk of not showing up. This evaluation is critical for informing operational decisions, such as implementing targeted patient outreach, optimizing scheduling, and reducing revenue loss associated with missed appointments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-markdown-1",
   "metadata": {},
   "source": [
    "## Methods: Data Loading and Preparation\n",
    "\n",
    "The first step is to load the necessary data by connecting to the SQL database. The following code fetches appointment details along with the corresponding no-show probability scores generated by the predictive model. This provides the foundational dataset for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646ee42d-d314-4b06-8e5a-c0a41e85a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "server = ######\n",
    "database = ######\n",
    "username = ###### \n",
    "password = ######\n",
    "\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "encoded_conn_str = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "# Create the SQLAlchemy engine \n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={encoded_conn_str}')\n",
    "\n",
    "query = '''\n",
    "SELECT \n",
    "    a.[mrn],\n",
    "    a.[csn],\n",
    "    a.[prc_name],\n",
    "    a.[department_name],\n",
    "    a.[dept_specialty_name],\n",
    "    a.[center_name],\n",
    "    a.[visit_provider_name],\n",
    "    a.[visit_prov_id],\n",
    "    a.[appt_status],\n",
    "    a.[days_to_appt] as appt_lead_days,\n",
    "    a.[appt_time],\n",
    "    a.[appt_date],   \n",
    "    a.[appt_length],\n",
    "    a.[appt_made_date],\n",
    "    p.[probability_percent],\n",
    "    p.[evaluation_utc_dttm]\n",
    "FROM \n",
    "    [rpt].[appointment] a\n",
    "INNER JOIN\n",
    "    [src].[appointment_predict] p\n",
    "    ON a.[csn] = p.[pat_enc_csn_id]\n",
    "'''\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-markdown-1",
   "metadata": {},
   "source": [
    "## Exploratory Analysis: Assessing Model Discrimination\n",
    "\n",
    "To begin our evaluation, we first explore the distribution of the model's predicted no-show probabilities across different appointment statuses. A boxplot is an effective way to visualize this.\n",
    "\n",
    "**Rationale:** If the model is performing well, we expect to see a clear separation in the predicted probabilities for different outcomes. Specifically, appointments that were 'No Show' or 'Canceled' should have significantly higher predicted no-show scores compared to those that were 'Completed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6ea047-a440-47a0-9c21-123e74b6270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.appt_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26062c0a-236f-44e6-a9cd-6e978538d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='appt_status', y='probability_percent')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of No Show Prediction by Appointment Status')\n",
    "plt.xlabel('Appointment Status')\n",
    "plt.ylabel('No Show Prediction Percent')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae458b9-f86f-45ad-bc20-a367924659b8",
   "metadata": {},
   "source": [
    "### Initial Observations from the Boxplot\n",
    "\n",
    "- **No Show & Canceled:** These categories show higher median prediction scores and a wider interquartile range (IQR). This is a positive sign, as it indicates the model is correctly assigning a higher risk of no-show to these appointments.\n",
    "- **Completed:** This group has a much lower median prediction score and a tighter distribution, which is the desired behavior. The model is generally confident that these patients will not be no-shows.\n",
    "- **Separation:** There is a clear visual separation between the distributions of the 'Completed' group and the 'No Show'/'Canceled' groups, suggesting the model has good discriminatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-class-markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation: Binary Classification Metrics\n",
    "\n",
    "To perform a more rigorous quantitative analysis, we will treat this as a binary classification problem. We define the \"positive\" class as appointments that are at high risk of not being completed (i.e., 'No Show', 'Canceled', 'Late Cancel'). All other outcomes will be considered the \"negative\" class.\n",
    "\n",
    "**Methods:**\n",
    "1.  **Filter Data:** Appointments with statuses like 'Arrived' or 'Scheduled' are excluded as their final outcomes are not yet determined or are ambiguous.\n",
    "2.  **Create Target Variable:** A binary `target` column is created, where `1` represents a high-risk outcome and `0` represents a completed appointment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25706cec-11c0-4d78-aa82-b63dc957ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance the prediction model as a binary classifiier\n",
    "# cannot include scheduled/arrived\n",
    "df = df[~df.appt_status.isin(['Arrived', 'Scheduled'])]\n",
    "df['target'] = df['appt_status'].isin(['No Show', 'Canceled', 'Late Cancel']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a84f7d05-ee13-4091-878f-572c6f3454bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.appt_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc-auc-markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Score\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) score is a key metric for evaluating a classifier's ability to distinguish between classes. It measures the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "- **Interpretation:**\n",
    "  - An AUC of **1.0** represents a perfect model.\n",
    "  - An AUC of **0.5** represents a model with no discriminatory ability (equivalent to random guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdbf3fd8-39a9-409c-b32d-2695398fd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(df['target'], df['probability_percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a59288-3a2a-4fa3-b652-d51b7847c2de",
   "metadata": {},
   "source": [
    "### What Does an ROC AUC of ~0.74 Mean?\n",
    "\n",
    "An ROC AUC score in the range of 0.7 to 0.8 is generally considered to indicate acceptable to good discrimination. Our score of **0.7385** suggests that:\n",
    "\n",
    "> \"There is a 73.85% chance that a randomly chosen patient who eventually cancels or no-shows will be assigned a higher predicted no-show probability than a randomly chosen patient who completes their appointment.\"\n",
    "\n",
    "This confirms that the model has a reasonably good ability to distinguish between high-risk and low-risk patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precision-recall-markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F1 Score at a Specific Threshold\n",
    "\n",
    "While ROC AUC evaluates the model across all possible thresholds, precision and recall assess its performance at a single, specific cutoff point. This is crucial for practical implementation, where a decision needs to be made (e.g., \"should we contact this patient?\").\n",
    "\n",
    "- **Precision:** Of all patients the model flagged as high-risk, what proportion actually were high-risk?\n",
    "- **Recall:** Of all patients who were truly high-risk, what proportion did the model correctly identify?\n",
    "- **F1 Score:** The harmonic mean of precision and recall, providing a single metric to balance the two.\n",
    "\n",
    "**Rationale:** The choice of threshold involves a trade-off. A low threshold increases recall (catching more no-shows) but lowers precision (more false alarms). A high threshold does the opposite. We will use a threshold of 0.3 (or 30%) for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e973ca26-b294-4751-90a2-2d33f2c860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.3  # or another value based on your business context\n",
    "df['predicted_class'] = df['probability_percent'] >= threshold\n",
    "\n",
    "precision = precision_score(df['target'], df['predicted_class'])\n",
    "recall = recall_score(df['target'], df['predicted_class'])\n",
    "f1 = f1_score(df['target'], df['predicted_class'])\n",
    "\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c748e-cc53-4736-a222-ebf880624de8",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "- **High recall (1.00):** This is a significant finding. At a 30% threshold, our model identifies **every single at-risk appointment**. This is excellent for minimizing the number of missed no-shows.\n",
    "\n",
    "- **Low precision (0.40):** The trade-off is that many of the appointments flagged as high-risk will not actually result in a no-show or cancellation. This means that if interventions (like reminder calls) are implemented for all flagged patients, some resources may be spent on patients who would have attended anyway.\n",
    "\n",
    "- **F1 Score (0.57):** This score reflects the balance between precision and recall. It indicates that the model, at this threshold, is heavily biased towards maximizing recall at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anova-markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing\n",
    "\n",
    "To further validate our findings, we will use statistical tests to determine if the observed differences in mean prediction scores across the different appointment status groups are statistically significant.\n",
    "\n",
    "**Methods:**\n",
    "1.  **ANOVA (Analysis of Variance):** This test checks if there is a significant difference in the means of two or more groups. A low p-value (typically < 0.05) suggests that at least one group mean is different from the others.\n",
    "2.  **Tukey HSD (Honestly Significant Difference) Test:** If the ANOVA test is significant, this post-hoc test performs pairwise comparisons between all groups to identify exactly which group means are different from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6513472-34f6-4960-9b5a-4527bb69a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure with ANOVA\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "groups = [group['probability_percent'] for name, group in df.groupby('appt_status')]\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "print(f\"ANOVA p-value: {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dde29e14-8555-48a0-b398-978d68420ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise comparison\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "tukey = pairwise_tukeyhsd(df['probability_percent'], df['appt_status'])\n",
    "print(tukey.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tukey-results-markdown",
   "metadata": {},
   "source": [
    "### Tukey HSD Test Results\n",
    "\n",
    "The results of the Tukey HSD test confirm that the model's predicted probabilities are statistically different across almost all appointment outcome groups (`reject = True`).\n",
    "\n",
    "- **Strongest Differentiation:** The largest mean difference is observed between the 'Completed' and 'Canceled' groups, and between 'Completed' and 'No Show'. This is excellent, as it provides strong statistical evidence that the model effectively separates patients who attend from those who do not.\n",
    "\n",
    "- **No Significant Difference:** The only pair that was not significantly different was 'Late Cancel' vs. 'Left without seen'. This may be due to a smaller sample size in these groups or because the underlying patient behaviors and risk factors are genuinely similar.\n",
    "\n",
    "**Conclusion from Statistical Tests:** The model's probability scores are not just visually different; they are statistically significant. This provides a high degree of confidence in using these scores for operational decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "calibration-analysis-markdown",
   "metadata": {},
   "source": [
    "## Calibration Analysis\n",
    "\n",
    "A well-calibrated model is one where the predicted probabilities are meaningful. For instance, if the model predicts a 20% no-show probability for a group of appointments, we would expect that approximately 20% of those appointments actually result in a no-show.\n",
    "\n",
    "**Method:** We can assess calibration by binning the predicted probabilities into deciles (10 groups) and then plotting the proportion of actual outcomes within each bin. A well-calibrated model will show a clear trend where the proportion of negative outcomes (like 'Canceled' and 'No Show') increases as the predicted probability bin increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88f45212-ddbf-4aad-95b1-db0d1438d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bin the probability percent and examine the outcome frequency per bin:\n",
    "df['prob_bin'] = pd.qcut(df['probability_percent'], q=10)\n",
    "\n",
    "bin_summary = df.groupby('prob_bin')['appt_status'].value_counts(normalize=True).unstack().fillna(0)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bin_summary.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Distribution of Appointment Status Across Prediction Percentile Bins')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "calibration-interpretation-markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Calibration Plot\n",
    "\n",
    "The stacked bar chart clearly demonstrates that the model is well-calibrated:\n",
    "\n",
    "- **Low-Risk Bins (Left Side):** In the bins with the lowest predicted probabilities, the dominant outcome is 'Completed' (orange). This is the desired behavior.\n",
    "- **High-Risk Bins (Right Side):** As we move to the bins with higher predicted probabilities, the proportion of 'Canceled' (blue) and 'No Show' (purple) appointments steadily increases.\n",
    "\n",
    "**Conclusion:** The model's predicted probabilities are not just abstract scores; they directly correlate with the real-world likelihood of a negative appointment outcome. This strong calibration makes the model highly interpretable and trustworthy for use in risk stratification and intervention planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings and Actionable Recommendations\n",
    "\n",
    "## Overall Model Performance\n",
    "- **Strong Discrimination:** The model demonstrates a good ability to distinguish between high-risk and low-risk appointments, with an ROC AUC score of approximately 0.74.\n",
    "- **Excellent Recall:** At a reasonable threshold, the model can identify nearly all potential no-shows and cancellations.\n",
    "- **Statistically Significant:** The differences in predicted probabilities across outcome groups are statistically significant, providing confidence in the model's output.\n",
    "- **Well-Calibrated:** The predicted probabilities are meaningful and align well with observed outcomes, making the model's output interpretable and actionable.\n",
    "\n",
    "## Actionable Recommendations\n",
    "Based on this evaluation, the following strategies can be implemented to leverage the model's predictive power:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations-markdown-1",
   "metadata": {},
   "source": [
    "### 1. Implement a Tiered Risk Intervention System\r\n",
    "\r\n",
    "Use prediction percentiles to assign patients into **low**, **moderate**, and **high** risk groups:\r\n",
    "\r\n",
    "| Risk Group     | Prediction Percent Range | Suggested Action                                                              |\r\n",
    "|----------------|---------------------------|--------------------------------------------------------------------------------|\r\n",
    "| Low Risk       | 0–12%                     | No extra action. Rely on standard communication (text, email).                |\r\n",
    "| Moderate Risk  | 12–25%                    | Send reminders closer to the date, offer rescheduling or telehealth.          |\r\n",
    "| High Risk      | >25%                      | Trigger personal outreach, rescheduling support, or transportation help.      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations-markdown-2",
   "metadata": {},
   "source": [
    "### 2. Build a Daily/Weekly \"At-Risk Appointments\" Dashboard\n",
    "\n",
    "Create a dashboard that lists upcoming appointments with high predicted no-show probabilities. This will enable staff to proactively manage these appointments by sorting, filtering, and taking appropriate action (e.g., confirmation calls).\n",
    "\n",
    "### 3. Analyze Root Causes in High-Risk Segments\n",
    "\n",
    "Use the model's predictions to segment high-risk patient populations. Analyze these segments by demographics, clinic location, and department to identify underlying patterns or operational inefficiencies that may be contributing to no-shows.\n",
    "\n",
    "### 4. Monitor and Retrain the Model\n",
    "\n",
    "Establish a monitoring pipeline to track the model's performance over time and measure the effectiveness of any interventions. The model should be retrained periodically (e.g., quarterly) to adapt to any shifts in patient behavior or clinical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialty-analysis-markdown",
   "metadata": {},
   "source": [
    "# Appendix: Specialty-Level Analysis\n",
    "\n",
    "To provide more granular insights, the following analysis breaks down the model's performance by department specialty. This can help identify specific clinical areas that may require more targeted intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d32b23b1-32b0-4bfb-a1f0-b4da34cb1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate appointment status by specialty\n",
    "dept_status_summary = df.groupby(['dept_specialty_name', 'appt_status'])['probability_percent'].describe()\n",
    "dept_status_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db762f4d-54fb-40c5-b9dc-cc53f3733a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot to compare mean risk by status per specialty\n",
    "pivot = df.pivot_table(\n",
    "    index='dept_specialty_name',\n",
    "    columns='appt_status',\n",
    "    values='probability_percent',\n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "pivot = pivot.sort_values(by='No Show', ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68a40533-f1e3-45dc-bc6e-c30ec478e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with heatmap\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivot, cmap='coolwarm', annot=True, fmt=\".1f\")\n",
    "plt.title(\"Mean No-Show Prediction % by Appointment Status and Specialty\")\n",
    "plt.ylabel(\"Specialty\")\n",
    "plt.xlabel(\"Appointment Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-interpretation-markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Heatmap of Mean No-Show Prediction % by Specialty\n",
    "\n",
    "The heatmap provides a clear visual summary of the model's risk assessment across different clinical specialties. \n",
    "\n",
    "---\n",
    "### Key Insights by Specialty\n",
    "\n",
    "| Specialty                | Notable Trends                                                                 |\n",
    "|--------------------------|---------------------------------------------------------------------------------|\n",
    "| **ENDO/MET/DIABV   | Shows extremely high predicted risk for 'Canceled' appointments (33.2%) and near-zero for all other outcomes. This anomaly suggests potential data quality issues or model confusion specific to this specialty and warrants further investigation. |\n",
    "| **NEUROLOGY / SURGERY NEURCAL** | Consistently high predicted risk for 'Canceled' appointments, indicating that the model is correctly identifying these specialties as being at higher risk for non-attendance. |\n",
    "| **PRIMARY CARE / PAIN MANAGEMENT** | These high-volume areas show moderately high predicted risk across all non-completed statuses. This presents a significant opportunity for implementing proactive scheduling and outreach interventions. |\n",
    "| **WOMENS HEALTH / INFUSION** | These specialties show the lowest predicted risk across most statuses, suggesting strong operational workflows or patient populations with a lower baseline no-show rate. |\n",
    "\n",
    "---\n",
    "\n",
    "### Actionable Recommendations\n",
    "\n",
    "1. **Investigate High-Risk Specialties:** Focus intervention efforts on specialties with the highest predicted risk, such as Endocrinology and Neurology. Strategies could include personalized reminders, appointment confirmation calls, or offering more flexible rescheduling options.\n",
    "\n",
    "2. **Review Data for Anomalous Specialties:** The unusual pattern for Endocrinology suggests a need to review the data pipeline and feature engineering for this specialty to ensure data quality and model accuracy.\n",
    "\n",
    "3. **Tailor Outreach Strategies:** Use these specialty-level insights to tailor patient communication. Low-risk departments can continue with standard automated messaging, while high-risk departments should consider more personalized and intensive outreach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
